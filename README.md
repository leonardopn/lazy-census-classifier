# lazy-census-classifier - Classificador de Renda com Racioc√≠nio Baseado em Casos (CBR)

Este projeto √© um classificador de aprendizado de m√°quina desenvolvido como parte da disciplina de P√≥s-Gradua√ß√£o em Aprendizado de M√°quina. O sistema utiliza a abordagem de Racioc√≠nio Baseado em Casos (CBR), um paradigma de _Lazy Learning_, para prever se a renda anual de um indiv√≠duo excede $50.000 com base em dados demogr√°ficos do censo.

O prot√≥tipo foi implementado em Python utilizando a biblioteca `cbrkit` e foca na aplica√ß√£o de conceitos te√≥ricos de CBR, como a constru√ß√£o de fun√ß√µes de similaridade h√≠bridas, pondera√ß√£o de atributos e otimiza√ß√£o do re√∫so com k-NN.

## üéØ Objetivo

O objetivo principal do trabalho √© construir um sistema CBR funcional que cumpra as etapas de **Recupera√ß√£o** e **Re√∫so** do ciclo CBR. A performance do sistema √© validada objetivamente atrav√©s do m√©todo de teste **Leave-One-Out Cross-Validation (LOOCV)**, conforme especificado nos requisitos da disciplina.

## ‚ú® Principais Funcionalidades

O sistema implementa v√°rias t√©cnicas para otimizar a precis√£o da classifica√ß√£o:

-   **Similaridade H√≠brida:** Utiliza diferentes fun√ß√µes de similaridade locais para cada tipo de atributo, combinando `similaridade linear` para dados num√©ricos (ex: idade) e `similaridade de igualdade` para dados categ√≥ricos (ex: escolaridade).
-   **Pondera√ß√£o de Atributos:** Aplica uma m√©dia ponderada para agregar as similaridades locais, permitindo que atributos mais relevantes (como `education` e `occupation`) tenham maior influ√™ncia no resultado final.
-   **Re√∫so com k-NN (k-Nearest Neighbors):** Em vez de usar apenas o caso mais similar (1-NN), o sistema recupera os `k` vizinhos mais pr√≥ximos e realiza uma vota√ß√£o majorit√°ria para determinar a classe final, tornando o classificador mais robusto a ru√≠dos.
-   **Avalia√ß√£o Paralelizada:** O teste de performance com _Leave-One-Out_ √© executado em paralelo utilizando `concurrent.futures`, acelerando significativamente o processo de valida√ß√£o.

## üìä Dataset

O projeto utiliza o dataset [**"Adult Census Income"**](https://www.kaggle.com/datasets/mosapabdelghany/adult-income-prediction-dataset), obtido do reposit√≥rio Kaggle. Ele cont√©m informa√ß√µes demogr√°ficas de mais de 30.000 indiv√≠duos e o objetivo √© classificar a coluna `income` em uma de duas categorias: `<=50K` ou `>50K`.

O CSV com os dados est√° localizado na pasta `datasets`.

## üõ†Ô∏è Tecnologias Utilizadas

-   **Linguagem:** Python 3.12+
-   **Biblioteca CBR:** `cbrkit`
-   **Manipula√ß√£o de Dados:** `pandas`
-   **Execu√ß√£o Paralela:** `concurrent.futures`
-   **Desenvolvimento:** `watchfiles` para recarregamento autom√°tico

## üöÄ Como Executar

1.  **Clone o reposit√≥rio:**

    ```bash
    git clone https://github.com/leonardopn/lazy-census-classifier
    cd lazy-census-classifier
    ```

2.  **Instale as depend√™ncias:**
    _O projeto utiliza [`uv`](https://docs.astral.sh/uv/) para gerenciamento de pacotes. [Clique aqui ](https://docs.astral.sh/uv/) para baixar o uv se for necess√°rio._

    ```bash
    uv run main.py
    ```

## üìà Resultados e An√°lise de Performance

Para validar a efic√°cia do classificador, foi conduzida uma s√©rie de testes utilizando o m√©todo _Leave-One-Out_ em uma amostra de 500 casos. As estrat√©gias testadas inclu√≠ram a varia√ß√£o do n√∫mero de vizinhos (k) no algoritmo k-NN e a aplica√ß√£o de pesos para os atributos.

### Resumo dos Resultados

A an√°lise dos testes revela uma jornada clara de otimiza√ß√£o e a intera√ß√£o complexa entre os hiperpar√¢metros do modelo:

1.  **Impacto do k-NN:** A mudan√ßa de um classificador 1-NN para um k-NN com mais vizinhos (k=5 e k=10) resultou nos ganhos de performance mais significativos. O salto de 1-NN para 5-NN (Teste 1 vs. Teste 3) aumentou a acur√°cia em quase 4%, provando que o sistema se torna muito mais robusto ao considerar m√∫ltiplos vizinhos para a vota√ß√£o da classe.
2.  **Efeito da Pondera√ß√£o:** A aplica√ß√£o de pesos nos atributos teve um resultado amb√≠guo. Ela melhorou a acur√°cia do modelo 1-NN, mas prejudicou ligeiramente os modelos com mais vizinhos (k=5 e k=10). Isso sugere que os pesos otimizados para encontrar o _√∫nico_ melhor vizinho n√£o s√£o necessariamente os ideais para encontrar o melhor _grupo_ de vizinhos, demonstrando a complexa intera√ß√£o entre as t√©cnicas de otimiza√ß√£o.
3.  **Ponto √ìtimo de Performance:** Os testes indicam que o ponto ideal para o modelo foi alcan√ßado no **Teste 5 (10-NN, sem pesos)**, atingindo **419 acertos** e uma acur√°cia m√°xima de **83.80%**.
4.  **In√≠cio da Perda:** Ao aumentar o n√∫mero de vizinhos para k=15, a performance come√ßou a decair consistentemente, indicando que a "vizinhan√ßa" se tornou muito ampla, incluindo casos menos relevantes na vota√ß√£o e diminuindo a capacidade de generaliza√ß√£o do modelo.

### Tabela Comparativa de Resultados

| Estrat√©gia do Teste            | N¬∫ de Acertos (de 500) |   Acur√°cia    | Melhora (vs. anterior) |
| ------------------------------ | :--------------------: | :-----------: | :--------------------: |
| Teste 1 (1-NN, sem pesos)      |          391           |    78.20%     |           -            |
| Teste 2 (1-NN, **com** pesos)  |          393           |    78.60%     |         +0.40%         |
| Teste 3 (5-NN, sem pesos)      |          410           |    82.00%     |       üëë +3.40%        |
| Teste 4 (5-NN, **com** pesos)  |          406           |    81.20%     |         -0.80%         |
| Teste 5 (10-NN, sem pesos)     |       **üëë 419**       | **üëë 83.80%** |       **+2.60%**       |
| Teste 6 (10-NN, **com** pesos) |          418           |    83.60%     |         -0.20%         |
| Teste 7 (15-NN, sem pesos)     |          415           |    83.00%     |         -0.60%         |
| Teste 8 (15-NN, **com** pesos) |          410           |    82.00%     |        üëé-1.00%        |
